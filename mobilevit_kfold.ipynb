{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPen5v8Q2I3X/3DQFK4mG8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69ef8b6bc5a04bd2938401f7677665d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8364372855a4b748c87fab4e0a9dbeb",
              "IPY_MODEL_6ef55d67ab554d22b102a40d3755c7e2",
              "IPY_MODEL_4dca1b6292f44c75b9c911fe6cc2706d"
            ],
            "layout": "IPY_MODEL_ff494f511ecd41a4a573423ad31d0c31"
          }
        },
        "e8364372855a4b748c87fab4e0a9dbeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a1656bfc2a4a7da3137950185a8c1b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c8e6064a3fe4bb485f5954191439747",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "6ef55d67ab554d22b102a40d3755c7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6dfac336fa4af593653f70260bed28",
            "max": 5540972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d72a6696d5148c28b77cfb5a49ae99f",
            "value": 5540972
          }
        },
        "4dca1b6292f44c75b9c911fe6cc2706d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00adfedf32b643d5bc1ce13671068e93",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c8203d3480a744f4a8e8c1b5e58f4d54",
            "value": "â€‡5.54M/5.54Mâ€‡[00:00&lt;00:00,â€‡69.8MB/s]"
          }
        },
        "ff494f511ecd41a4a573423ad31d0c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29a1656bfc2a4a7da3137950185a8c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8e6064a3fe4bb485f5954191439747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a6dfac336fa4af593653f70260bed28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d72a6696d5148c28b77cfb5a49ae99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00adfedf32b643d5bc1ce13671068e93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8203d3480a744f4a8e8c1b5e58f4d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Htets-Corner/SYNTHBUSTER_RAISE-1k/blob/main/mobilevit_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eVNzRlhRgJK",
        "outputId": "b9e8158f-6dcf-4979-f822-6b0c678c8d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Real images path: /content/drive/MyDrive/RAISE/PNG\n",
            "AI images path: /content/drive/MyDrive/synthbuster\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Step 0: Data Preparation\n",
        "# ==========================\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define dataset paths\n",
        "import os\n",
        "real_path = \"/content/drive/MyDrive/RAISE/PNG\"\n",
        "ai_path   = \"/content/drive/MyDrive/synthbuster\"\n",
        "\n",
        "print(\"Real images path:\", real_path)\n",
        "print(\"AI images path:\", ai_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Collect all file paths with labels\n",
        "valid_exts = ('.jpg', '.jpeg', '.png', '.PNG', '.bmp', '.tif', '.tiff', '.webp')\n",
        "\n",
        "# Real images\n",
        "real_files = [os.path.join(real_path, f) for f in os.listdir(real_path)\n",
        "              if f.lower().endswith(valid_exts)]\n",
        "\n",
        "# AI images (inside multiple subfolders)\n",
        "ai_files = []\n",
        "for subfolder in os.listdir(ai_path):\n",
        "    sub_path = os.path.join(ai_path, subfolder)\n",
        "    if os.path.isdir(sub_path):\n",
        "        ai_files.extend([os.path.join(sub_path, f) for f in os.listdir(sub_path)\n",
        "                         if f.lower().endswith(valid_exts)])\n",
        "\n",
        "print(f\"Found {len(real_files)} real images\")\n",
        "print(f\"Found {len(ai_files)} AI images\")\n",
        "\n",
        "# 4. Build dataframe of filepaths + labels\n",
        "import pandas as pd\n",
        "df_real = pd.DataFrame({\"filepath\": real_files, \"label\": \"real\"})\n",
        "df_ai   = pd.DataFrame({\"filepath\": ai_files, \"label\": \"ai\"})\n",
        "df = pd.concat([df_real, df_ai], ignore_index=True)\n",
        "\n",
        "print(\"Total dataset size:\", len(df))\n",
        "print(df[\"label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqUejoiKT8KM",
        "outputId": "268ed91e-0a46-447c-c5da-e6824a83ab0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 999 real images\n",
            "Found 3000 AI images\n",
            "Total dataset size: 3999\n",
            "label\n",
            "ai      3000\n",
            "real     999\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define transforms (resize + normalization for MobileViT)\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "image_size = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 6. Define custom Dataset for dataframe\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.label_map = {\"real\": 0, \"ai\": 1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.loc[idx, \"filepath\"]\n",
        "        label = self.label_map[self.df.loc[idx, \"label\"]]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = ImageDataset(df, transform=transform)\n",
        "\n",
        "# 7. K-Fold Split with WeightedRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "n_splits = 10   # Number of folds\n",
        "batch_size = 32\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "labels = df[\"label\"].map({\"real\": 0, \"ai\": 1}).values\n",
        "folds = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df, labels)):\n",
        "    print(f\"\\nðŸ”¹ Fold {fold+1}/{n_splits}\")\n",
        "    print(\"Train size:\", len(train_idx), \"Val size:\", len(val_idx))\n",
        "\n",
        "    # Train / Val subsets\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset   = Subset(full_dataset, val_idx)\n",
        "\n",
        "    # Class balance in training subset\n",
        "    train_labels = labels[train_idx]\n",
        "    counts = Counter(train_labels)\n",
        "    class_weights = {cls: 1.0/count for cls, count in counts.items()}\n",
        "    sample_weights = np.array([class_weights[l] for l in train_labels])\n",
        "\n",
        "    # WeightedRandomSampler\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, sampler=sampler,\n",
        "                              num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_subset, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=2, pin_memory=True)\n",
        "\n",
        "    folds.append({\n",
        "        \"fold\": fold,\n",
        "        \"train_idx\": train_idx,\n",
        "        \"val_idx\": val_idx,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader\n",
        "    })\n",
        "\n",
        "print(\"\\nâœ… All folds prepared. Access them as folds[i]['train_loader'] and folds[i]['val_loader']\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcm4ZNVfUcZ3",
        "outputId": "0c3a36e8-3a50-4a5e-9d8d-d5e6df6d0405"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ Fold 1/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 2/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 3/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 4/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 5/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 6/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 7/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 8/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 9/10\n",
            "Train size: 3599 Val size: 400\n",
            "\n",
            "ðŸ”¹ Fold 10/10\n",
            "Train size: 3600 Val size: 399\n",
            "\n",
            "âœ… All folds prepared. Access them as folds[i]['train_loader'] and folds[i]['val_loader']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Step 1: Model Setup\n",
        "# ==========================\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Choose model\n",
        "model_name = \"mobilevitv2_050\"\n",
        "\n",
        "# 2. Function to create a fresh model (for each fold)\n",
        "def create_model(model_name=\"mobilevitv2_050\", num_classes=2, device=None):\n",
        "    # Load pretrained MobileViT\n",
        "    model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "    # Reset classifier head for binary classification\n",
        "    if hasattr(model, \"reset_classifier\"):\n",
        "        model.reset_classifier(num_classes=num_classes)\n",
        "    else:\n",
        "        # fallback if reset_classifier not available\n",
        "        in_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    # Move to device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"âœ… Loaded {model_name} with classifier reset to {num_classes} classes\")\n",
        "    return model, device\n",
        "\n",
        "# 3. Detect device once\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 4. Example: build one model\n",
        "model, device = create_model(model_name=model_name, num_classes=2, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "69ef8b6bc5a04bd2938401f7677665d2",
            "e8364372855a4b748c87fab4e0a9dbeb",
            "6ef55d67ab554d22b102a40d3755c7e2",
            "4dca1b6292f44c75b9c911fe6cc2706d",
            "ff494f511ecd41a4a573423ad31d0c31",
            "29a1656bfc2a4a7da3137950185a8c1b",
            "8c8e6064a3fe4bb485f5954191439747",
            "6a6dfac336fa4af593653f70260bed28",
            "9d72a6696d5148c28b77cfb5a49ae99f",
            "00adfedf32b643d5bc1ce13671068e93",
            "c8203d3480a744f4a8e8c1b5e58f4d54"
          ]
        },
        "id": "JVPWRXZZWYTA",
        "outputId": "a5e71022-4d27-4555-b3a1-42622d88db6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.54M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69ef8b6bc5a04bd2938401f7677665d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded mobilevitv2_050 with classifier reset to 2 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============= CONFIG =============\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-4\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints_kfold\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============= TRAINING FUNCTION =============\n",
        "def train_one_fold(model, train_loader, val_loader, fold, start_epoch=0):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [],\n",
        "               \"precision\": [], \"recall\": [], \"f1\": []}\n",
        "\n",
        "    # Checkpoint path for this fold\n",
        "    fold_ckpt = os.path.join(checkpoint_dir, f\"mobilevit_fold{fold}.pth\")\n",
        "\n",
        "    # Resume training if checkpoint exists\n",
        "    if os.path.exists(fold_ckpt):\n",
        "        print(f\"ðŸ”„ Resuming Fold {fold} from checkpoint...\")\n",
        "        checkpoint = torch.load(fold_ckpt, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "        best_val_acc = checkpoint[\"best_val_acc\"]\n",
        "        history = checkpoint[\"history\"]\n",
        "        start_epoch = checkpoint[\"epoch\"] + 1\n",
        "        print(f\"âœ… Resumed from epoch {start_epoch}, best_val_acc={best_val_acc:.4f}\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        print(f\"\\nðŸ“˜ Fold {fold} | Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        # ---------- TRAIN ----------\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "        for imgs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * imgs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        train_loss /= total\n",
        "\n",
        "        # ---------- VALIDATION ----------\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        all_labels, all_preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * imgs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "        val_loss /= total\n",
        "\n",
        "        # Compute metrics\n",
        "        precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
        "        recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
        "        f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
        "\n",
        "        # Save history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"precision\"].append(precision)\n",
        "        history[\"recall\"].append(recall)\n",
        "        history[\"f1\"].append(f1)\n",
        "\n",
        "        print(f\"ðŸ“Š Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"ðŸ“Š Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
        "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        # Save checkpoint (every epoch)\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_val_acc\": best_val_acc,\n",
        "            \"history\": history,\n",
        "        }\n",
        "        torch.save(checkpoint, fold_ckpt)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            print(\"ðŸ’¾ Saving best model for this fold...\")\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"best_fold{fold}.pth\"))\n",
        "\n",
        "    return history\n",
        "\n",
        "# ============= K-FOLD TRAINING LOOP =============\n",
        "def train_kfold(model_fn, train_loaders, val_loaders, k_folds):\n",
        "    fold_histories = []\n",
        "    for fold in range(k_folds):\n",
        "        print(f\"\\nðŸ”¹ Starting Fold {fold+1}/{k_folds}\")\n",
        "\n",
        "        # Fresh model per fold\n",
        "        model = model_fn().to(device)\n",
        "\n",
        "        history = train_one_fold(model, train_loaders[fold], val_loaders[fold], fold)\n",
        "        fold_histories.append(history)\n",
        "\n",
        "    # Compute average metrics across folds\n",
        "    avg_metrics = {key: sum(h[key][-1] for h in fold_histories)/k_folds\n",
        "                   for key in fold_histories[0].keys()}\n",
        "\n",
        "    print(\"\\nâœ… K-Fold Training Completed!\")\n",
        "    print(f\"ðŸ“Š Avg Val Acc: {avg_metrics['val_acc']:.2f}%, \"\n",
        "          f\"Avg Precision: {avg_metrics['precision']:.4f}, \"\n",
        "          f\"Avg Recall: {avg_metrics['recall']:.4f}, \"\n",
        "          f\"Avg F1: {avg_metrics['f1']:.4f}\")\n",
        "\n",
        "    return fold_histories, avg_metrics\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BTECPA-kcGHx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# FULL PIPELINE: K-Fold MobileViT Training\n",
        "# =========================================\n",
        "\n",
        "# -------------------------------\n",
        "# Step 0: Data Preparation (K-Fold)\n",
        "# -------------------------------\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from PIL import Image\n",
        "\n",
        "# Paths to original datasets\n",
        "real_path = \"/content/drive/MyDrive/RAISE/PNG\"\n",
        "ai_path = \"/content/drive/MyDrive/synthbuster\"\n",
        "\n",
        "# -------------------------------\n",
        "# Custom Dataset\n",
        "# -------------------------------\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, files, label, transform=None):\n",
        "        self.files = files\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.label\n",
        "\n",
        "# -------------------------------\n",
        "# Gather files and create dataset\n",
        "# -------------------------------\n",
        "image_size = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "# Real images\n",
        "real_files = glob.glob(os.path.join(real_path, \"*.png\"))\n",
        "real_dataset = CustomImageDataset(real_files, 1, transform)\n",
        "\n",
        "# AI images (multiple subfolders)\n",
        "ai_files = []\n",
        "for sub in os.listdir(ai_path):\n",
        "    sub_path = os.path.join(ai_path, sub)\n",
        "    if os.path.isdir(sub_path):\n",
        "        ai_files += glob.glob(os.path.join(sub_path, \"*.png\"))\n",
        "ai_dataset = CustomImageDataset(ai_files, 0, transform)\n",
        "\n",
        "# Combine datasets\n",
        "full_dataset = real_dataset + ai_dataset  # Dataset supports concatenation in newer PyTorch\n",
        "\n",
        "# -------------------------------\n",
        "# K-Fold Split\n",
        "# -------------------------------\n",
        "k_folds = 10\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "folds = []\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(full_dataset)))):\n",
        "    # Subsets\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    # WeightedRandomSampler for class imbalance\n",
        "    targets = [full_dataset[i][1] for i in train_idx]\n",
        "    from collections import Counter\n",
        "    counts = Counter(targets)\n",
        "    class_weights = {cls: 1.0 / count for cls, count in counts.items()}\n",
        "    sample_weights = [class_weights[t] for t in targets]\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    # Dataloaders\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    #folds.append({\"fold\": fold, \"train_loader\": train_loader, \"val_loader\": val_loader})\n",
        "\n",
        "folds.append({\n",
        "        \"fold\": fold,\n",
        "        \"train_idx\": train_idx,\n",
        "        \"val_idx\": val_idx,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader\n",
        "    })\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ri99R9PGqz-4",
        "outputId": "2940668f-bf15-4ddb-f784-678a9a18e2a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3191633323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# WeightedRandomSampler for class imbalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3191633323.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2319\u001b[0m                 )\n\u001b[1;32m   2320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract train_loaders and val_loaders\n",
        "train_loaders = [f['train_loader'] for f in folds]\n",
        "val_loaders   = [f['val_loader'] for f in folds]\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: MobileViT Model Setup\n",
        "# -------------------------------\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def create_mobilevit():\n",
        "    model_name = \"mobilevitv2_050\"\n",
        "    model = timm.create_model(model_name, pretrained=True)\n",
        "    if hasattr(model, \"reset_classifier\"):\n",
        "        model.reset_classifier(num_classes=2)\n",
        "    else:\n",
        "        in_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(in_features, 2)\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: K-Fold Training with Early Stopping + Resume\n",
        "# -------------------------------\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import json\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/kfold_checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\"\"\"\n",
        "def train_one_fold(model, device, train_loader, val_loader, fold, num_epochs=10, patience=3):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [],\n",
        "               \"precision\": [], \"recall\": [], \"f1\": []}\n",
        "\n",
        "    fold_dir = os.path.join(checkpoint_dir, f\"fold_{fold}\")\n",
        "    os.makedirs(fold_dir, exist_ok=True)\n",
        "    model_path = os.path.join(fold_dir, \"best_model.pth\")\n",
        "    history_path = os.path.join(fold_dir, \"history.json\")\n",
        "\n",
        "    if os.path.exists(model_path) and os.path.exists(history_path):\n",
        "        print(f\"â© Resuming Fold {fold} from checkpoint...\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        with open(history_path, \"r\") as f:\n",
        "            history = json.load(f)\n",
        "        return history, model\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * imgs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
        "        recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
        "        f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
        "\n",
        "        # Save history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"precision\"].append(precision)\n",
        "        history[\"recall\"].append(recall)\n",
        "        history[\"f1\"].append(f1)\n",
        "\n",
        "        print(f\"Fold {fold} | Epoch {epoch+1} | Train Acc {train_acc:.2f}% | Val Acc {val_acc:.2f}% | F1 {f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            with open(history_path, \"w\") as f:\n",
        "                json.dump(history, f)\n",
        "            print(\"ðŸ’¾ Saved new best model!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"â¹ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    return history, model \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# -------------------------------\n",
        "# Updated train_one_fold with tqdm progress bars\n",
        "# -------------------------------\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_one_fold(model, device, train_loader, val_loader, fold, num_epochs=10, patience=3):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [],\n",
        "               \"precision\": [], \"recall\": [], \"f1\": []}\n",
        "\n",
        "    fold_dir = os.path.join(checkpoint_dir, f\"fold_{fold}\")\n",
        "    os.makedirs(fold_dir, exist_ok=True)\n",
        "    model_path = os.path.join(fold_dir, \"best_model.pth\")\n",
        "    history_path = os.path.join(fold_dir, \"history.json\")\n",
        "\n",
        "    # Resume if checkpoint exists\n",
        "    if os.path.exists(model_path) and os.path.exists(history_path):\n",
        "        print(f\"â© Resuming Fold {fold} from checkpoint...\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        with open(history_path, \"r\") as f:\n",
        "            history = json.load(f)\n",
        "        return history, model\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "        loop = tqdm(train_loader, desc=f\"Fold {fold} Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for imgs, labels in loop:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            loop.set_postfix(loss=running_loss/total, acc=100*correct/total)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
        "        recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
        "        f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
        "\n",
        "        # Save history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"precision\"].append(precision)\n",
        "        history[\"recall\"].append(recall)\n",
        "        history[\"f1\"].append(f1)\n",
        "\n",
        "        print(f\"Fold {fold} | Epoch {epoch+1} | Train Acc {train_acc:.2f}% | Val Acc {val_acc:.2f}% | F1 {f1:.4f}\")\n",
        "\n",
        "        # ---- Early stopping + save best model ----\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            with open(history_path, \"w\") as f:\n",
        "                json.dump(history, f)\n",
        "            print(\"ðŸ’¾ Saved new best model!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"â¹ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # ---- Confusion Matrix ----\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['AI','Real'], yticklabels=['AI','Real'])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(f\"Fold {fold} Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return history, model\n",
        "\n",
        "\n",
        "def train_kfold(create_model_fn, train_loaders, val_loaders, k_folds=5, num_epochs=10, patience=3):\n",
        "    all_histories = []\n",
        "    fold_metrics = {\"val_acc\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for fold in range(k_folds):\n",
        "        print(f\"\\n========== Fold {fold} ==========\")\n",
        "        model = create_model_fn().to(device)\n",
        "        history, model = train_one_fold(model, device, train_loaders[fold], val_loaders[fold],\n",
        "                                        fold, num_epochs=num_epochs, patience=patience)\n",
        "        all_histories.append(history)\n",
        "\n",
        "        fold_metrics[\"val_acc\"].append(max(history[\"val_acc\"]))\n",
        "        fold_metrics[\"precision\"].append(max(history[\"precision\"]))\n",
        "        fold_metrics[\"recall\"].append(max(history[\"recall\"]))\n",
        "        fold_metrics[\"f1\"].append(max(history[\"f1\"]))\n",
        "\n",
        "    avg_metrics = {k: sum(v)/len(v) for k,v in fold_metrics.items()}\n",
        "    return all_histories, avg_metrics\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2.5: Run K-Fold Training\n",
        "# -------------------------------\n",
        "fold_histories, avg_metrics = train_kfold(create_mobilevit, train_loaders, val_loaders, k_folds)\n",
        "print(\"\\nðŸ“Œ Average Metrics Across Folds:\")\n",
        "print(avg_metrics)\n",
        "\"\"\"\n",
        "\n",
        "import os, copy, sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ðŸ”¹ Training function for one fold\n",
        "def train_one_fold(fold, model_fn, train_loader, val_loader, num_epochs=10,\n",
        "                   patience=3, best_model_dir=\"/content/drive/MyDrive/Dataset/kfold_results\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model_fn().to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    os.makedirs(best_model_dir, exist_ok=True)\n",
        "    best_model_path = os.path.join(best_model_dir, f\"mobilevitv2_fold{fold}_best.pth\")\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {\n",
        "        \"epoch\": [], \"train_loss\": [], \"train_acc\": [],\n",
        "        \"val_loss\": [], \"val_acc\": [], \"val_macro_f1\": [], \"val_bal_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ---------------- TRAIN ----------------\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Fold {fold} Epoch {epoch+1}/{num_epochs} [Train]\",\n",
        "                    file=sys.stdout, dynamic_ncols=True)\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            loop.set_postfix(loss=running_loss/total, acc=100*correct/total)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # ---------------- VALIDATION ----------------\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        val_macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        val_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        # Save history\n",
        "        history[\"epoch\"].append(epoch+1)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"val_macro_f1\"].append(val_macro_f1)\n",
        "        history[\"val_bal_acc\"].append(val_bal_acc)\n",
        "\n",
        "        print(f\"âœ… Fold {fold} | Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | \"\n",
        "              f\"Macro-F1: {val_macro_f1:.4f} | BalAcc: {val_bal_acc:.4f}\",\n",
        "              flush=True)\n",
        "\n",
        "        # ---------------- EARLY STOPPING ----------------\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            patience_counter = 0\n",
        "            print(f\"ðŸ’¾ Best model saved for Fold {fold} at {best_model_path}\", flush=True)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"â¹ï¸ Early stopping at epoch {epoch+1} for Fold {fold}\", flush=True)\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---------------- CONFUSION MATRIX ----------------\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"AI\", \"Real\"], yticklabels=[\"AI\", \"Real\"])\n",
        "    plt.title(f\"Confusion Matrix - Fold {fold}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    return history, best_val_acc\n",
        "\n",
        "\n",
        "# ðŸ”¹ Full K-Fold Loop\n",
        "def train_kfold(model_fn, folds, num_epochs=10, patience=3):\n",
        "    all_histories, fold_metrics = [], []\n",
        "\n",
        "    for fold, fold_data in enumerate(folds):\n",
        "        print(f\"\\nðŸš€ Starting Fold {fold+1}/{len(folds)}\")\n",
        "        train_loader, val_loader = fold_data['train_loader'], fold_data['val_loader']\n",
        "\n",
        "        history, best_val_acc = train_one_fold(\n",
        "            fold=fold+1,\n",
        "            model_fn=model_fn,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=num_epochs,\n",
        "            patience=patience\n",
        "        )\n",
        "\n",
        "        all_histories.append(history)\n",
        "        fold_metrics.append(best_val_acc)\n",
        "\n",
        "    avg_acc = sum(fold_metrics) / len(fold_metrics)\n",
        "    print(f\"\\nðŸ“Š Average Val Accuracy across folds: {avg_acc:.2f}%\")\n",
        "\n",
        "    return all_histories, fold_metrics, avg_acc\n",
        "\n",
        "\n",
        "# ðŸš€ Run K-Fold Training\n",
        "fold_histories, fold_metrics, avg_metrics = train_kfold(create_mobilevit, folds, num_epochs=10, patience=3)\n"
      ],
      "metadata": {
        "id": "ksJyWV6voZGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Step 3: Results Visualization\n",
        "# -------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_fold_curves(history, fold):\n",
        "    epochs = range(1, len(history[\"train_loss\"])+1)\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.title(f\"Fold {fold} Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.title(f\"Fold {fold} Accuracy\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_fold_metrics(history, fold):\n",
        "    epochs = range(1, len(history[\"f1\"])+1)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(epochs, history[\"precision\"], label=\"Precision\")\n",
        "    plt.plot(epochs, history[\"recall\"], label=\"Recall\")\n",
        "    plt.plot(epochs, history[\"f1\"], label=\"F1 Score\")\n",
        "    plt.title(f\"Fold {fold} Metrics\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_cv_summary(all_histories, avg_metrics):\n",
        "    num_folds = len(all_histories)\n",
        "    best_acc = [max(h[\"val_acc\"]) for h in all_histories]\n",
        "    best_prec = [max(h[\"precision\"]) for h in all_histories]\n",
        "    best_recall = [max(h[\"recall\"]) for h in all_histories]\n",
        "    best_f1 = [max(h[\"f1\"]) for h in all_histories]\n",
        "\n",
        "    x = np.arange(num_folds); width=0.2\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.bar(x-0.3, best_acc, width, label=\"Accuracy\")\n",
        "    plt.bar(x-0.1, best_prec, width, label=\"Precision\")\n",
        "    plt.bar(x+0.1, best_recall, width, label=\"Recall\")\n",
        "    plt.bar(x+0.3, best_f1, width, label=\"F1\")\n",
        "    plt.xticks(x, [f\"Fold {i}\" for i in range(num_folds)])\n",
        "    plt.title(\"Cross-Validation Metrics per Fold\")\n",
        "    plt.ylabel(\"Score\"); plt.legend(); plt.show()\n",
        "\n",
        "# Example usage:\n",
        "for fold_id, history in enumerate(fold_histories):\n",
        "    plot_fold_curves(history, fold_id)\n",
        "    plot_fold_metrics(history, fold_id)\n",
        "\n",
        "plot_cv_summary(fold_histories, avg_metrics)"
      ],
      "metadata": {
        "id": "eyeI81EfoN9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LVo39DrrRDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KygFQ72rrQrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Setup & Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, ConcatDataset\n",
        "from torchvision import transforms, datasets\n",
        "import timm\n"
      ],
      "metadata": {
        "id": "3Un1PhmGrQjl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dataset & K-Fold Data Preparation (Fixed for your structure)\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, real_path, ai_path, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Real images = class 0\n",
        "        real_files = glob.glob(os.path.join(real_path, \"*.png\"))\n",
        "        self.images.extend(real_files)\n",
        "        self.labels.extend([0] * len(real_files))\n",
        "\n",
        "        # AI images = class 1 (all subfolders)\n",
        "        ai_files = glob.glob(os.path.join(ai_path, \"**/*.png\"), recursive=True)\n",
        "        self.images.extend(ai_files)\n",
        "        self.labels.extend([1] * len(ai_files))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = CustomImageDataset(real_path=\"/content/drive/MyDrive/RAISE/PNG\",\n",
        "                             ai_path=\"/content/drive/MyDrive/synthbuster\",\n",
        "                             transform=transform)\n",
        "\n",
        "print(f\"âœ… Loaded dataset with {len(dataset)} images\")\n",
        "print(f\"   Real: {sum(np.array(dataset.labels)==0)}, AI: {sum(np.array(dataset.labels)==1)}\")\n",
        "\n",
        "# Extract labels\n",
        "targets = np.array(dataset.labels)\n",
        "\n",
        "# K-Fold split\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "train_loaders, val_loaders = [], []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
        "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "\n",
        "    # Compute class weights for sampler\n",
        "    class_sample_counts = np.bincount(targets[train_idx])\n",
        "    weights = 1. / class_sample_counts\n",
        "    samples_weights = weights[targets[train_idx]]\n",
        "\n",
        "    sampler = WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=32, sampler=sampler)\n",
        "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "    train_loaders.append(train_loader)\n",
        "    val_loaders.append(val_loader)\n",
        "\n",
        "print(f\"âœ… Prepared {k_folds}-Fold DataLoaders\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SICFf6_0rW6Z",
        "outputId": "7d0974be-c3b5-4e8a-c017-e97c2af7bc21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded dataset with 3999 images\n",
            "   Real: 999, AI: 3000\n",
            "âœ… Prepared 5-Fold DataLoaders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Model Setup\n",
        "\n",
        "def create_mobilevit():\n",
        "    model_name = \"mobilevitv2_050\"\n",
        "    model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "    if hasattr(model, \"reset_classifier\"):\n",
        "        model.reset_classifier(num_classes=2)\n",
        "    else:\n",
        "        in_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(in_features, 2)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    return model, device\n",
        "\n",
        "print(\"âœ… Model setup function ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqIkXw6NssBd",
        "outputId": "0841a176-5afa-4fc1-86d7-84f5d85dcf34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model setup function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Training Loop\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    acc = correct / total\n",
        "    prec = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    rec = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "    return running_loss / total, acc, prec, rec, f1, all_labels, all_preds\n",
        "\n",
        "\n",
        "def train_kfold(create_model_fn, train_loaders, val_loaders, k_folds=5, num_epochs=10, patience=3):\n",
        "    all_fold_metrics = []\n",
        "    histories = []\n",
        "\n",
        "    for fold in range(k_folds):\n",
        "        print(f\"\\nðŸš€ Training Fold {fold+1}/{k_folds}\")\n",
        "        model, device = create_model_fn()\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "        best_val_loss = np.inf\n",
        "        patience_counter = 0\n",
        "        history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nðŸ“Œ Fold {fold+1}, Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            train_loss, train_acc = train_one_epoch(model, train_loaders[fold], criterion, optimizer, device)\n",
        "            val_loss, val_acc, val_prec, val_rec, val_f1, y_true, y_pred = validate(model, val_loaders[fold], criterion, device)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"val_loss\"].append(val_loss)\n",
        "            history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "            print(f\"   Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "            print(f\"   Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "            # Save best model per fold\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pth\")\n",
        "                print(\"   âœ… Best model saved\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"   â¹ï¸ Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        disp = ConfusionMatrixDisplay(cm, display_labels=[\"Real\", \"AI\"])\n",
        "        disp.plot(cmap=\"Blues\")\n",
        "        plt.title(f\"Confusion Matrix - Fold {fold+1}\")\n",
        "        plt.show()\n",
        "\n",
        "        all_fold_metrics.append({\"acc\": val_acc, \"prec\": val_prec, \"rec\": val_rec, \"f1\": val_f1})\n",
        "        histories.append(history)\n",
        "\n",
        "    avg_metrics = {\n",
        "        \"acc\": np.mean([m[\"acc\"] for m in all_fold_metrics]),\n",
        "        \"prec\": np.mean([m[\"prec\"] for m in all_fold_metrics]),\n",
        "        \"rec\": np.mean([m[\"rec\"] for m in all_fold_metrics]),\n",
        "        \"f1\": np.mean([m[\"f1\"] for m in all_fold_metrics]),\n",
        "    }\n",
        "\n",
        "    return histories, avg_metrics\n"
      ],
      "metadata": {
        "id": "vglg6f_tsu0I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Run Training\n",
        "fold_histories, avg_metrics = train_kfold(create_mobilevit, train_loaders, val_loaders, k_folds=5, num_epochs=10, patience=3)\n",
        "print(\"\\nâœ… Training complete!\")\n",
        "print(\"Average Metrics:\", avg_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s97CMNSgs2xR",
        "outputId": "0837ab33-48a0-4485-aa39-d973c0bd2d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Œ Fold 1, Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Train Loss: 0.6552, Acc: 0.6386\n",
            "   Val   Loss: 0.6029, Acc: 0.7662, Prec: 0.7986, Rec: 0.7662, F1: 0.7761\n",
            "   âœ… Best model saved\n",
            "\n",
            "ðŸ“Œ Fold 1, Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Train Loss: 0.5014, Acc: 0.8024\n",
            "   Val   Loss: 0.3821, Acc: 0.8738, Prec: 0.8762, Rec: 0.8738, F1: 0.8748\n",
            "   âœ… Best model saved\n",
            "\n",
            "ðŸ“Œ Fold 1, Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|â–ˆâ–        | 14/100 [04:43<29:14, 20.40s/it]"
          ]
        }
      ]
    }
  ]
}