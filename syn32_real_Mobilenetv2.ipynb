{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG40m0HuH99dUrcxVLF6BD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Htets-Corner/SYNTHBUSTER_RAISE-1k/blob/main/syn32_real_Mobilenetv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya4-iw9hWtzH",
        "outputId": "593cea8c-61d2-4a25-f767-83a65f02b311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Mount Drive and Import Libraries\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import core libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Torch and torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Utilities\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/"
      ],
      "metadata": {
        "id": "twWyIu8oaOYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"drive/MyDrive/RAISE/PNG\"\n",
        "# Count only .png files\n",
        "png_count = sum(1 for f in os.listdir(folder_path) if f.lower().endswith(\".png\"))\n",
        "\n",
        "print(f\"Number of PNG files in '{folder_path}': {png_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WGylBSadJ-c",
        "outputId": "b1d57d6e-04e7-4253-dd7e-faed74f31751"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PNG files in 'drive/MyDrive/RAISE/PNG': 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Datasets and Split into Train/Test\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download Synthbuster (JPEG resized version) from Kaggle\n",
        "synth_path = kagglehub.dataset_download(\"devpatel484/synthbuster-32\")\n",
        "print(\"Path to Synthbuster dataset:\", synth_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBqv4WL0alXW",
        "outputId": "1402ff2b-068e-4c4e-8cb3-aa072fd22e4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/devpatel484/synthbuster-32?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.04M/9.04M [00:00<00:00, 106MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to Synthbuster dataset: /root/.cache/kagglehub/datasets/devpatel484/synthbuster-32/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "real_path = \"/content/drive/MyDrive/RAISE/PNG\"   # Real images\n",
        "ai_path   = synth_path   # AI images (JPEGs)\n",
        "\n",
        "# Define transforms (resize, normalize, augment for train)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # MobileNetV2 expects 224x224\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Combine both datasets under one structure using ImageFolder\n",
        "# Let's first create parent folders with 'real' and 'ai' subdirs\n",
        "\n",
        "!mkdir -p /content/dataset/real\n",
        "!mkdir -p /content/dataset/ai\n",
        "\n",
        "# Symlink instead of copying (saves space)\n",
        "!ln -s \"{real_path}\"/* /content/dataset/real/\n",
        "!ln -s \"{ai_path}\"/* /content/dataset/ai/\n",
        "\n",
        "# Now create dataset with ImageFolder\n",
        "full_dataset = datasets.ImageFolder(\n",
        "    root=\"/content/dataset\",\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "# Train/Test split (80/20)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size  = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, test_size]\n",
        ")\n",
        "\n",
        "# Apply correct transforms\n",
        "train_dataset.dataset.transform = transform_train\n",
        "test_dataset.dataset.transform  = transform_test\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
        "print(f\"Classes: {full_dataset.classes}\")"
      ],
      "metadata": {
        "id": "RZ15uFH1a4af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls dataset/ai/resized_data_Synthbuster/Synthbuster_Dataset/\n",
        "!ls dataset/real"
      ],
      "metadata": {
        "id": "htU-DCTedkLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Develop MobileNetV2 Model for Binary Classification\n",
        "\n",
        "# Load pretrained MobileNetV2\n",
        "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Freeze feature extractor (optional: comment out if you want full fine-tuning)\n",
        "for param in mobilenet_v2.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify classifier for 2 classes (real vs AI)\n",
        "mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.last_channel, 2)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mobilenet_v2 = mobilenet_v2.to(device)\n",
        "\n",
        "# Define loss function & optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mobilenet_v2.parameters(), lr=0.001)\n",
        "\n",
        "#print(mobilenet_v2)\n"
      ],
      "metadata": {
        "id": "-O8fY9iMeAPk",
        "outputId": "6ca0f432-dff0-42ac-9dec-5e41995de315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:00<00:00, 143MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Training Loop with Accuracy, Loss Tracking & Saving Best Model\n",
        "\n",
        "num_epochs = 20\n",
        "best_acc = 0.0\n",
        "\n",
        "# Store results for visualization\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # -------------------- Training --------------------\n",
        "    mobilenet_v2.train()\n",
        "    running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mobilenet_v2(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataset)\n",
        "    epoch_train_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "    # -------------------- Evaluation --------------------\n",
        "    mobilenet_v2.eval()\n",
        "    test_loss, test_corrects = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = mobilenet_v2(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_test_loss = test_loss / len(test_dataset)\n",
        "    epoch_test_acc = test_corrects.double() / len(test_dataset)\n",
        "\n",
        "    # Save results\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    test_losses.append(epoch_test_loss)\n",
        "    train_accs.append(epoch_train_acc.item())\n",
        "    test_accs.append(epoch_test_acc.item())\n",
        "\n",
        "    # Save best model\n",
        "    if epoch_test_acc > best_acc:\n",
        "        best_acc = epoch_test_acc\n",
        "        torch.save(mobilenet_v2.state_dict(), \"best_mobilenetv2.pth\")\n",
        "        print(\"ðŸ’¾ Best model saved\")\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2%} \"\n",
        "          f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "QyZmJmEzozzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}